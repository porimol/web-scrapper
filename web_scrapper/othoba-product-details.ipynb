{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a5d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from random import randint\n",
    "import time\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import scrapy\n",
    "from scrapy_selenium import SeleniumRequest\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OthobaSpider(scrapy.Spider):\n",
    "    \"\"\"\n",
    "    A Scrapy spider for scraping data from the Othoba website.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): The name of the spider.\n",
    "        allowed_domains (list): The list of allowed domains for the spider.\n",
    "        start_urls (list): The list of start URLs for the spider.\n",
    "        visited_urls (set): Set to store visited product URLs.\n",
    "        df (list): The list to store the scraped data.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, name=None, **kwargs): Initializes the spider.\n",
    "        start_requests(self): Generates the initial request to start scraping.\n",
    "        parse(self, response): Parses the response and extracts the desired data.\n",
    "        scrape_data(self, driver): Scrapes the data from each product element.\n",
    "        closed(self, reason): Handles the spider closed event and saves the scraped data to a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"othoba\"\n",
    "    allowed_domains = [\"www.othoba.com\"]\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the lists and sets required for scraping.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name of the spider.\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.start_urls = [\"https://www.othoba.com/smartphone?orderby=0&pagesize=40\"]\n",
    "        self.visited_urls = set()\n",
    "        self.df = []\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"\n",
    "        Generates the initial request to start scraping.\n",
    "\n",
    "        Returns:\n",
    "            SeleniumRequest: The initial request to start scraping.\n",
    "        \"\"\"\n",
    "        for url in self.start_urls:\n",
    "            yield SeleniumRequest(\n",
    "                url=url,\n",
    "                callback=self.parse,\n",
    "                wait_time=randint(8, 12),\n",
    "                wait_until=EC.presence_of_element_located((By.CLASS_NAME, 'pagination'))\n",
    "            )\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        Parses the response and extracts the desired data.\n",
    "\n",
    "        Args:\n",
    "            response (scrapy.http.Response): The response object.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        driver = response.request.meta['driver']\n",
    "        self.scrape_data(driver)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                next_page_element = driver.find_element(By.XPATH, '//li[@class=\"page-item active\"]/following-sibling::li/a')\n",
    "                next_page_url = next_page_element.get_attribute(\"href\")\n",
    "                print(f\"Next Page's URL: {next_page_url}\")\n",
    "                if next_page_url not in self.visited_urls:\n",
    "                    self.visited_urls.add(next_page_url)\n",
    "                    driver.get(next_page_url)\n",
    "                    self.scrape_data(driver)\n",
    "                else:\n",
    "                    print(\"All mobile details link scrapped.\")\n",
    "                    break\n",
    "                self.visited_urls.add(next_page_url)\n",
    "            except NoSuchElementException:\n",
    "                print(\"Scrapping completed.\")\n",
    "                break\n",
    "            time.sleep(randint(3, 7))\n",
    "\n",
    "    def scrape_data(self, driver):\n",
    "        \"\"\"\n",
    "        Scrapes the data from each product element on the current page.\n",
    "\n",
    "        Args:\n",
    "            driver (selenium.webdriver.Chrome): The Selenium WebDriver instance.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # lambda function to extract numbers using regex from a string\n",
    "        extract_number = lambda element: re.sub(r\"[^\\d,]\", \"\", element).strip() if element else None\n",
    "\n",
    "        # Select all product elements and iterate over them\n",
    "        for product in driver.find_elements(By.XPATH, '//*[@class=\"product product-image-gap product-simple\"]'):\n",
    "            # Scrape the desired data from each product\n",
    "            product_name_link = product.find_element(By.CLASS_NAME, \"product-name\")\n",
    "            product_details_link = product_name_link.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "            if product_details_link not in self.visited_urls:\n",
    "                self.visited_urls.add(product_details_link)\n",
    "                # Call BeautifulSoup to get the product details\n",
    "                req = requests.get(product_details_link, timeout=30)\n",
    "                # Passing the requested content to Beautiful Soup\n",
    "                product_soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "                product_title = product_soup.find(\"h1\", class_=\"product-title\").text\n",
    "                dom = etree.HTML(str(product_soup))\n",
    "\n",
    "                # If product specifications are not available, skip the product\n",
    "                if dom.xpath('//*[@id=\"product-tab-specification\"]/ul/li') == []:\n",
    "                    continue\n",
    "\n",
    "                ram = dom.xpath('//*[@id=\"product-tab-specification\"]/ul/li[2]/p')[0].text\n",
    "                ram = re.sub(r\"[^\\d\\w,]\", \" \", ram).strip()\n",
    "\n",
    "                storage_dom = dom.xpath('//*[@id=\"product-tab-specification\"]/ul/li[3]/p')\n",
    "                storage = None\n",
    "                if storage_dom:\n",
    "                    storage = storage_dom[0].text.strip()\n",
    "\n",
    "                display_dom = dom.xpath('//*[@id=\"product-tab-specification\"]/ul/li[4]/p')\n",
    "                display = None\n",
    "                if display_dom:\n",
    "                    display = display_dom[0].text.strip()\n",
    "\n",
    "                phone_color_dom = dom.xpath('//*[@id=\"product-tab-specification\"]/ul/li[1]/p')\n",
    "                phone_color = None\n",
    "                if phone_color_dom:\n",
    "                    phone_color = phone_color_dom[0].text.strip()\n",
    "\n",
    "                operating_system_dom = dom.xpath('//*[@id=\"product-tab-specification\"]/ul/li[5]/p')\n",
    "                operating_system = None\n",
    "                if operating_system_dom:\n",
    "                    operating_system = operating_system_dom[0].text.strip()\n",
    "\n",
    "                current_price = product.find_element(By.XPATH, '//*[@class=\"new-price dl-new-price-product\"]').text\n",
    "                if current_price:\n",
    "                    # current_price = re.sub(r\"[^\\d,]\", \"\", current_price).strip()\n",
    "                    current_price = extract_number(current_price)\n",
    "\n",
    "                old_price = product.find_element(By.XPATH, '//*[@class=\"old-price\"]').text\n",
    "                if old_price:\n",
    "                    # old_price = re.sub(r\"[^\\d,]\", \"\", old_price).strip()\n",
    "                    old_price = extract_number(old_price)\n",
    "\n",
    "                # avg_rating = dom.xpath('//*[@id=\"product-details-form\"]/div/div[3]/div/span[2]')[0].text\n",
    "                total_reviews = dom.xpath('//*[@id=\"product-details-form\"]/div/div[3]/a')[0].text\n",
    "                # total_reviews = re.sub(r\"[^\\d,]\", \"\", total_reviews)\n",
    "                total_reviews = extract_number(total_reviews)\n",
    "\n",
    "                brand_name = dom.xpath('//*[@id=\"product-details-form\"]/div/div[1]/div/div[2]/div[2]/span[2]/a')[0].text\n",
    "                seller = dom.xpath('//*[@id=\"product-details-form\"]/div/div[1]/div/div[2]/div[3]/span[2]/a')[0].text\n",
    "                seller_rating = dom.xpath('//*[@class=\"sold-info seller-statting\"]/p[2]')[0].text\n",
    "                ship_on_time = dom.xpath('//*[@class=\"sold-info seller-ship\"]/p[2]')[0].text\n",
    "\n",
    "                result = {\n",
    "                    \"product_details_link\": product_details_link,\n",
    "                    \"product_title\": product_title,\n",
    "                    \"ram\": ram,\n",
    "                    \"storage\": storage,\n",
    "                    \"display\": display,\n",
    "                    \"phone_color\": phone_color,\n",
    "                    \"operating_system\": operating_system,\n",
    "                    \"current_price\": current_price,\n",
    "                    \"old_price\": old_price,\n",
    "                    # \"avg_rating\": avg_rating,\n",
    "                    \"total_reviews\": total_reviews,\n",
    "                    \"brand_name\": brand_name,\n",
    "                    \"seller\": seller,\n",
    "                    \"seller_rating\": seller_rating,\n",
    "                    \"ship_on_time\": ship_on_time\n",
    "                }\n",
    "                pprint(f\"\\n{result}\")\n",
    "\n",
    "                # append the scraped data to the list of scraped items\n",
    "                self.df.append(result)\n",
    "                break\n",
    "\n",
    "    def closed(self, reason):\n",
    "        \"\"\"\n",
    "        Handles the spider closed event and saves the scraped data to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            reason (str): The reason why the spider was closed.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The scraped data as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        parent_dir = os.path.dirname(os.getcwd())\n",
    "        dataset_path = os.path.join(parent_dir, \"datasets\", \"othoba_products.csv\")\n",
    "        df = pd.DataFrame(self.df)\n",
    "        df.to_csv(dataset_path, index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3138b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  Create an instance of the spider\n",
    "spider = OthobaSpider()\n",
    "\n",
    "# Execute the spider\n",
    "df = spider.closed(reason=\"Finished\")\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38dd778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "\n",
    "\n",
    "#  # Create a CrawlerProcess\n",
    "# process = CrawlerProcess(get_project_settings())\n",
    "# # Run the spider and pass the callback function\n",
    "# process.crawl(OthobaSpider)\n",
    "# process.start()\n",
    "# process.stop()\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(OthobaSpider)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5e49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3f250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
